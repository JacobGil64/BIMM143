---
title: "class07 machine learning 1"
author-meta: "Jacob Gil"
author: "Jacob Gil"
format: pdf
---

## Clustering 

with kmeans()

```{r}

x <- rnorm(10000, mean = 3)
hist(x)
#x

```

```{r}
tmp <- c(rnorm(30, mean = 3), rnorm(30, -3))
tmp

x <- cbind(x = tmp, y = rev(tmp))
head(x)

plot(x)

k <- kmeans(x, centers = 2, nstart = 20)
k
```
>Q1. How many points in each cluster

```{r}
k$size
```

>Q2. CLuster membership?

```{r}
k$cluster
```

>Q3. Cluster centers?

```{r}
k$centers
```

>Q4. Plot my cluster

```{r}
plot(x, col=k$cluster, pch = 16)
```

>Q5. Cluster the data into 4 groups with kmeans and plot

```{r}
k4 <- kmeans(x, centers = 4, nstart = 20)
k4
plot(x, col=k4$cluster, pch = 16)

```
kmeans is popular because it is fast and straightforward. It is limited by knowledge of clusters.

# Hierarchical clustering

the main functions is called hclust() that is passed in a distance matri (dist())

```{r}
hc <- hclust(dist(x))
hc
plot(hc)

```

to find clusters use cutree 

```{r}
plot(hc)
abline(h=8, col = "red")
grps <- cutree(hc, h=8)

table(grps)
```
>Q6. Plot hclust result

```{r}
plot(hc, grps, pch = 16)

```

#Lab

```{r}
url <- "https://tinyurl.com/UK-foods"
x <- read.csv(url)
x
dim(x)

# Note how the minus indexing works
#rownames(x) <- x[,1]
#x <- x[,-1]
#head(x)


#' instead of ^^ use
x <- read.csv(url, row.names=1)
head(x)
dim(x)
```
>Q2. 

the row.names=1 approach is better because it is harder to mess up the data in the matrix (ex. if you ran x <- x[,-1] multiple times it would start deleting your data)

```{r}
barplot(as.matrix(x), beside=T, col=rainbow(nrow(x)))

barplot(as.matrix(x), beside=F, col=rainbow(nrow(x)))
```
>Q3.

changing the barplot beside value to false

```{r}
pairs(x, col=rainbow(17), pch=16)
```

>Q5.

the diagonal halves are identical. The y axis relates to the country that column and the x axis relates to the country that is on that row.

>Q6.

the green and blue dots in the middle of the graphs, and the pink dot on the bottom lefts are the furthest from the axis.

```{r}
# Use the prcomp() PCA function 
pca <- prcomp( t(x) )
summary(pca)
```
```{r}
# Plot PC1 vs PC2
plot(pca$x[,1], pca$x[,2], xlab="PC1", ylab="PC2", xlim=c(-270,500))
text(pca$x[,1], pca$x[,2], colnames(x), col=c("orange","red", "blue", "green"))
```
the loadings tell us how much he original variables conribute to the new variables (the PCs)
```{r}
v <- round( pca$sdev^2/sum(pca$sdev^2) * 100 )
v
barplot(v, xlab="Principal Component", ylab="Percent Variation")
```


```{r}
head(pca$rotation)
## Lets focus on PC1 as it accounts for > 90% of variance 
par(mar=c(10, 3, 0.35, 0))
barplot( pca$rotation[,1], las=2 )
```

>Q9.

```{r}
head(pca$rotation)
## Lets focus on PC1 as it accounts for > 90% of variance 
par(mar=c(10, 3, 0.35, 0))
barplot( pca$rotation[,2], las=2 )
```

